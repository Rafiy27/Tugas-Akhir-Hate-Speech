{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FSilb-tTSmBQ"
      ],
      "authorship_tag": "ABX9TyOo0ru0YbDr2LQM1JOwkyGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafiy27/Tugas-Akhir-Hate-Speech/blob/main/Corpus_Building_Feature_Extraction_(FastText)_Feature_Expansion_(TF_IDF)_Tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Library"
      ],
      "metadata": {
        "id": "FSilb-tTSmBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V0-USUvMuv6",
        "outputId": "5e3fef4f-3b14-413f-e4cb-f41c9523e58c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XESbuwC-Nd5j",
        "outputId": "beb6f42e-5393-4979-fe56-d511e3a1b9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=b69f859b327ba82cf30b486f3ca9dbf190b285fd388e708387ba8e2bd4ecdcdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Preprocess Data Tweet"
      ],
      "metadata": {
        "id": "_r5v2dUETZI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library & Load Dataset"
      ],
      "metadata": {
        "id": "-pZdmmP4UrJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load the dataset from Excel\n",
        "df = pd.read_excel('modified_dataset - Copy - Copy.xlsx', engine='openpyxl')"
      ],
      "metadata": {
        "id": "LwiJIBWcM0Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows where 'full_text' is not a string\n",
        "error_rows = df[df['full_text'].apply(lambda x: not isinstance(x, str))]\n",
        "\n",
        "# Display the rows with the error\n",
        "print(error_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IREgy-2_tKVo",
        "outputId": "474e8120-d477-4a25-fc95-0b27a379e397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         No.               id_str  conversation_id_str full_text  Label_Putri  \\\n",
            "8927    8928  1716550000000000000  1716050000000000000       NaN          NaN   \n",
            "16514  16515             1,65E+18             1,65E+18       NaN          NaN   \n",
            "16515  16516             1,65E+18             1,65E+18       NaN          NaN   \n",
            "16516  16517             1,65E+18             1,65E+18       NaN          NaN   \n",
            "16517  16518             1,65E+18             1,65E+18       NaN          NaN   \n",
            "16518  16519             1,65E+18             1,65E+18       NaN          NaN   \n",
            "16519  16520             1,65E+18             1,65E+18       NaN          NaN   \n",
            "\n",
            "       Label_Dea  Label_Rafi  Label_Final  \n",
            "8927         NaN         0.0            0  \n",
            "16514        NaN         NaN            0  \n",
            "16515        NaN         NaN            1  \n",
            "16516        NaN         NaN            0  \n",
            "16517        NaN         NaN            0  \n",
            "16518        NaN         NaN            1  \n",
            "16519        NaN         NaN            1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['full_text'], inplace=True)"
      ],
      "metadata": {
        "id": "onW2X-nTtZIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data Tweet"
      ],
      "metadata": {
        "id": "ZCaf4yGyUuPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-word characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove emojis\n",
        "    return text\n",
        "\n",
        "df['full_text'] = df['full_text'].apply(clean_text)\n",
        "\n",
        "# Case Folding\n",
        "df['full_text'] = df['full_text'].str.lower()\n",
        "\n",
        "# Tokenizing\n",
        "nltk.download('punkt')\n",
        "df['tokens'] = df['full_text'].apply(word_tokenize)\n",
        "\n",
        "# Filtering with Stop Words\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['filtered_tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "df['stemmed_text'] = df['filtered_tokens'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x]))\n",
        "\n",
        "# Extracting labels (assuming they are in a column named 'Label_Final')\n",
        "labels = df['Label_Final']\n",
        "\n",
        "# Display the processed data (optional)\n",
        "print(df[['full_text', 'stemmed_text', 'Label_Final']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q12kStQeUpcf",
        "outputId": "ff9af7ca-ba1e-456e-cdb3-3fa95228733f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           full_text  \\\n",
            "0               android_ak_47 anies orang campus aja   \n",
            "1  pak anies juga blm masa kampanye tapi bawaslu ...   \n",
            "2  kosim__ yang ngusung anies sbg gub dki itu pks...   \n",
            "3  android_ak_47 saya malu klo tdk milih anies me...   \n",
            "4  msaid_didu entah siapa yg salah ttg kegagalan ...   \n",
            "\n",
            "                                        stemmed_text  Label_Final  \n",
            "0                  android_ak_47 ani orang campu aja            0  \n",
            "1  ani blm kampany bawaslu sdh edar sm berantai m...            0  \n",
            "2  kosim__ ngusung ani sbg gub dki pk gerindrapra...            0  \n",
            "3  android_ak_47 malu klo tdk milih ani kerusakan...            0  \n",
            "4  msaid_didu yg salah ttg kegagalan food estat y...            0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Corpus"
      ],
      "metadata": {
        "id": "rs4vfGeCTdY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all 'stemmed_text' entries to create a corpus\n",
        "corpus = ' '.join(df['stemmed_text'].astype(str))\n",
        "\n",
        "# Save the corpus to a text file (optional)\n",
        "with open('corpus.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(corpus)\n",
        "\n",
        "print(\"Corpus created and saved as 'corpus.txt'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2jz0M8MPm5y",
        "outputId": "d594bb7b-e622-4ebf-86e8-d0ec174b3c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus created and saved as 'corpus.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train FastText Model"
      ],
      "metadata": {
        "id": "DqpMsxDeTgEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Path to the corpus file\n",
        "corpus_path = 'corpus.txt'\n",
        "\n",
        "# Output path for the trained FastText model\n",
        "model_path = 'trained_model.bin'\n",
        "\n",
        "# Train the FastText model\n",
        "# Here, we're using default parameters. You can adjust them based on your requirements.\n",
        "model = fasttext.train_unsupervised(corpus_path, model='skipgram')\n",
        "\n",
        "# Save the trained model\n",
        "model.save_model(model_path)\n",
        "\n",
        "print(\"Model trained and saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vp8D7NzNWBq",
        "outputId": "7a22aa1a-2457-4458-8f0f-edf4eea7877d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained FastText model\n",
        "model = fasttext.load_model(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVwuyae4V_l4",
        "outputId": "eadf83cb-809b-468d-ac95-392ab34b3765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ekstraksi Fitur"
      ],
      "metadata": {
        "id": "oRMNTC0VWC9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract feature vectors for a given text\n",
        "def extract_features(text):\n",
        "    words = text.split()\n",
        "    feature_vectors = []\n",
        "    for word in words:\n",
        "        vector = model.get_word_vector(word)\n",
        "        feature_vectors.append(vector)\n",
        "    return feature_vectors\n",
        "\n",
        "# Apply feature extraction to the 'stemmed_text' column\n",
        "df['feature_vectors'] = df['stemmed_text'].apply(extract_features)\n",
        "\n",
        "# Convert the list of feature vectors into a single array for each row\n",
        "df['feature_vectors'] = df['feature_vectors'].apply(np.array)\n",
        "\n",
        "# Save the DataFrame with feature vectors to a CSV file (optional)\n",
        "df.to_csv('data_with_features.csv', index=False)\n",
        "\n",
        "# Display the DataFrame with feature vectors (optional)\n",
        "print(df[['full_text', 'stemmed_text', 'Label_Final', 'feature_vectors']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZeWwpHEV9lq",
        "outputId": "f9755b85-668e-47e3-8a13-9f67ec4bf879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           full_text  \\\n",
            "0               android_ak_47 anies orang campus aja   \n",
            "1  pak anies juga blm masa kampanye tapi bawaslu ...   \n",
            "2  kosim__ yang ngusung anies sbg gub dki itu pks...   \n",
            "3  android_ak_47 saya malu klo tdk milih anies me...   \n",
            "4  msaid_didu entah siapa yg salah ttg kegagalan ...   \n",
            "\n",
            "                                        stemmed_text  Label_Final  \\\n",
            "0                  android_ak_47 ani orang campu aja            0   \n",
            "1  ani blm kampany bawaslu sdh edar sm berantai m...            0   \n",
            "2  kosim__ ngusung ani sbg gub dki pk gerindrapra...            0   \n",
            "3  android_ak_47 malu klo tdk milih ani kerusakan...            0   \n",
            "4  msaid_didu yg salah ttg kegagalan food estat y...            0   \n",
            "\n",
            "                                     feature_vectors  \n",
            "0  [[0.07310548, -0.019598916, 0.30345085, 0.1652...  \n",
            "1  [[0.0022707172, 0.028685242, -0.18997926, -0.2...  \n",
            "2  [[-0.41321686, -0.32017013, -0.16772135, -0.20...  \n",
            "3  [[0.07310548, -0.019598916, 0.30345085, 0.1652...  \n",
            "4  [[0.23604497, -0.060167056, 0.3833863, -0.1188...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ekspansi Fitur (TF IDF)"
      ],
      "metadata": {
        "id": "IwN8dLTJfmfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TF-IDF vectorizer with max_features set to 5000\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the 'stemmed_text' to compute TF-IDF scores\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['stemmed_text'])\n",
        "\n",
        "# Convert the TF-IDF matrix to a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Combine the original DataFrame with the TF-IDF DataFrame\n",
        "df_combined = pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "# Save the combined DataFrame to a CSV file (optional)\n",
        "df_combined.to_csv('data_with_features_and_tfidf.csv', index=False)\n",
        "\n",
        "# Display the combined DataFrame (optional)\n",
        "print(df_combined[['full_text', 'stemmed_text', 'Label_Final'] + list(tfidf_vectorizer.get_feature_names_out())].head())"
      ],
      "metadata": {
        "id": "Kh1Fg9w1fo2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775dc88e-4c91-4352-c6ef-d0b5d2d88467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           full_text  \\\n",
            "0               android_ak_47 anies orang campus aja   \n",
            "1  pak anies juga blm masa kampanye tapi bawaslu ...   \n",
            "2  kosim__ yang ngusung anies sbg gub dki itu pks...   \n",
            "3  android_ak_47 saya malu klo tdk milih anies me...   \n",
            "4  msaid_didu entah siapa yg salah ttg kegagalan ...   \n",
            "\n",
            "                                        stemmed_text  Label_Final   01   02  \\\n",
            "0                  android_ak_47 ani orang campu aja          0.0  0.0  0.0   \n",
            "1  ani blm kampany bawaslu sdh edar sm berantai m...          0.0  0.0  0.0   \n",
            "2  kosim__ ngusung ani sbg gub dki pk gerindrapra...          0.0  0.0  0.0   \n",
            "3  android_ak_47 malu klo tdk milih ani kerusakan...          0.0  0.0  0.0   \n",
            "4  msaid_didu yg salah ttg kegagalan food estat y...          0.0  0.0  0.0   \n",
            "\n",
            "   03__nakula   10  100  1000   11  ...  zioni  zoelfick  zoey  zon  zonauang  \\\n",
            "0         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "1         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "2         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "3         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "4         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "\n",
            "    zu  zul  zulfiarrahman  zulha  zulkifli  \n",
            "0  0.0  0.0            0.0    0.0       0.0  \n",
            "1  0.0  0.0            0.0    0.0       0.0  \n",
            "2  0.0  0.0            0.0    0.0       0.0  \n",
            "3  0.0  0.0            0.0    0.0       0.0  \n",
            "4  0.0  0.0            0.0    0.0       0.0  \n",
            "\n",
            "[5 rows x 5003 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the combined DataFrame (optional)\n",
        "print(df_combined[['full_text', 'stemmed_text', 'Label_Final'] + list(tfidf_vectorizer.get_feature_names_out())].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LobkNv6whffW",
        "outputId": "d0d74c5f-58b4-472a-b904-33db1411b049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           full_text  \\\n",
            "0               android_ak_47 anies orang campus aja   \n",
            "1  pak anies juga blm masa kampanye tapi bawaslu ...   \n",
            "2  kosim__ yang ngusung anies sbg gub dki itu pks...   \n",
            "3  android_ak_47 saya malu klo tdk milih anies me...   \n",
            "4  msaid_didu entah siapa yg salah ttg kegagalan ...   \n",
            "\n",
            "                                        stemmed_text  Label_Final   01   02  \\\n",
            "0                  android_ak_47 ani orang campu aja          0.0  0.0  0.0   \n",
            "1  ani blm kampany bawaslu sdh edar sm berantai m...          0.0  0.0  0.0   \n",
            "2  kosim__ ngusung ani sbg gub dki pk gerindrapra...          0.0  0.0  0.0   \n",
            "3  android_ak_47 malu klo tdk milih ani kerusakan...          0.0  0.0  0.0   \n",
            "4  msaid_didu yg salah ttg kegagalan food estat y...          0.0  0.0  0.0   \n",
            "\n",
            "   03__nakula   10  100  1000   11  ...  zioni  zoelfick  zoey  zon  zonauang  \\\n",
            "0         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "1         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "2         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "3         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "4         0.0  0.0  0.0   0.0  0.0  ...    0.0       0.0   0.0  0.0       0.0   \n",
            "\n",
            "    zu  zul  zulfiarrahman  zulha  zulkifli  \n",
            "0  0.0  0.0            0.0    0.0       0.0  \n",
            "1  0.0  0.0            0.0    0.0       0.0  \n",
            "2  0.0  0.0            0.0    0.0       0.0  \n",
            "3  0.0  0.0            0.0    0.0       0.0  \n",
            "4  0.0  0.0            0.0    0.0       0.0  \n",
            "\n",
            "[5 rows x 5003 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model information\n",
        "print(f\"Dimension of word vectors: {model.get_dimension()}\")\n",
        "print(f\"Number of words in the model's vocabulary: {len(model.words)}\")\n",
        "\n",
        "# Display a subset of the vocabulary (first 10 words as an example)\n",
        "print(\"\\nSample vocabulary:\")\n",
        "for word in model.words[:10]:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGyIen3HNzvw",
        "outputId": "67e4bceb-5f41-440e-9ca0-3b0b53fb1a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of word vectors: 100\n",
            "Number of words in the model's vocabulary: 10790\n",
            "\n",
            "Sample vocabulary:\n",
            "polisi\n",
            "yg\n",
            "rt\n",
            "agama\n",
            "ani\n",
            "bangsat\n",
            "kontol\n",
            "lu\n",
            "jokowi\n",
            "orang\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_nearest_neighbors('Anies'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGFBTbBXOaNM",
        "outputId": "4b0d35d5-598b-44fa-f1c5-e77a8984f5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.9315925240516663, 'aniessandi'), (0.931538462638855, 'aniesngibul'), (0.9249277710914612, 'aniespenipu'), (0.9236127138137817, 'aniesgaben'), (0.9234165549278259, 'anies_relawan'), (0.9165322184562683, 'aniesmania'), (0.9150654673576355, 'aniesbusuk'), (0.9149628281593323, 'aniesimin'), (0.9147285223007202, 'aniesgabecu'), (0.9102646708488464, 'aniesygmani')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors and words\n",
        "words = model.words\n",
        "vectors = [model.get_word_vector(word) for word in words]\n",
        "\n",
        "# Create a DataFrame to store the word vectors\n",
        "word_vectors_df = pd.DataFrame(np.array(vectors), index=words)\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "word_vectors_df.to_csv('word_vectors.csv')\n",
        "\n",
        "# Display the DataFrame (optional)\n",
        "print(word_vectors_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIYQqONJRym5",
        "outputId": "a7eac188-57f4-44dc-de0f-badd3e108409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              0         1         2         3         4         5         6   \\\n",
            "polisi -0.576942 -0.616024  0.342153 -0.397687 -0.030046 -0.608560 -0.352871   \n",
            "yg      0.062726 -0.180544  0.290070  0.164792 -0.065855  0.049383 -0.099984   \n",
            "rt      0.111445 -0.157175  0.368534  0.368823  0.162012  0.143876  0.496929   \n",
            "agama   0.351173  0.219019  0.698730  0.411099  0.049002 -0.078647  0.833379   \n",
            "ani     0.002271  0.028685 -0.189979 -0.276333 -0.221277  0.140011 -0.878774   \n",
            "\n",
            "              7         8         9   ...        90        91        92  \\\n",
            "polisi -0.088243  0.589864 -0.656664  ...  0.559057  0.546212  0.026423   \n",
            "yg     -0.152045  0.199241 -0.373167  ...  0.319712 -0.169090 -0.031436   \n",
            "rt      0.025692  0.085010 -0.146897  ...  0.003236 -0.264650  0.134863   \n",
            "agama   0.058797  0.138839 -0.136421  ...  0.091312  0.099400  0.194005   \n",
            "ani     0.244312  0.541722  0.349854  ...  1.138707 -0.103604 -0.012713   \n",
            "\n",
            "              93        94        95        96        97        98        99  \n",
            "polisi -0.016324  0.375961  0.441461 -0.015124  0.277882 -0.260068 -0.311383  \n",
            "yg     -0.078263 -0.060749  0.250459  0.070652 -0.133176 -0.006915 -0.040534  \n",
            "rt      0.389073 -0.059480  0.405740  0.233593 -0.086152 -0.048328  0.119177  \n",
            "agama   0.710091 -0.142792  0.354420  0.226261  0.085915  0.021597  0.224584  \n",
            "ani    -0.291660 -0.044767 -0.484806 -0.073299  0.265010  0.311106  0.283142  \n",
            "\n",
            "[5 rows x 100 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xaZdWOC6Uibz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}